{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e8892-c1d5-48c8-825b-b53113f3eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely.vectorized\n",
    "import os\n",
    "\n",
    "# Crea una cartella per salvare i file NetCDF scaricati\n",
    "output_dir = \"Italia_data\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Imposta il client CDS API\n",
    "client = cdsapi.Client(url='https://ads.atmosphere.copernicus.eu/api',\n",
    "                       key='6d4e1c06-ba15-4e08-a729-9f43ce76a3c1')\n",
    "\n",
    "# Parametri fissi per la richiesta\n",
    "dataset = \"cams-europe-air-quality-forecasts\"\n",
    "# Lista di tutte le ore della giornata (dalle 00:00 alle 23:00)\n",
    "hours = [f\"{i:02d}:00\" for i in range(24)]\n",
    "# Area geografica per l'Italia: [nord, ovest, sud, est]\n",
    "area = [47.1, 6.6, 36.6, 18.5]\n",
    "\n",
    "# Suddividi l'anno 2024 in periodi mensili\n",
    "months = pd.date_range(\"2024-01-01\", \"2024-12-01\", freq=\"MS\")\n",
    "for m in months:\n",
    "    start_date = m.strftime(\"%Y-%m-%d\")\n",
    "    end_date = (m + pd.offsets.MonthEnd(0)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    request = {\n",
    "        'variable': [ \n",
    "            'particulate_matter_2.5um', \n",
    "            'particulate_matter_10um',\n",
    "            'carbon_monoxide',\n",
    "            'nitrogen_dioxide',\n",
    "            'sulphur_dioxide'\n",
    "        ],\n",
    "        'model': ['ensemble'],\n",
    "        'level': ['0'],\n",
    "        'date': [f'{start_date}/{end_date}'],\n",
    "        'type': ['analysis'],\n",
    "        'time': hours,  # tutte le ore della giornata\n",
    "        'leadtime_hour': ['0'],\n",
    "        'data_format': 'netcdf',\n",
    "        'area': area\n",
    "    }\n",
    "    \n",
    "    filename = os.path.join(output_dir, f\"Italia_{m.strftime('%Y_%m')}.nc\")\n",
    "    print(f\"Recupero dati dal {start_date} al {end_date}...\")\n",
    "    client.retrieve(dataset, request).download(filename)\n",
    "\n",
    "# Aggrega tutti i file NetCDF in un unico dataset\n",
    "nc_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".nc\")]\n",
    "combined_data = xr.open_mfdataset(nc_files, combine='by_coords')\n",
    "\n",
    "# Funzione per aggregare i dati per regioni NUTS-3 (o livello desiderato) usando la media\n",
    "def aggregate_by_nuts3(dataset, nuts3_gdf, variable_name='pm10_conc'):\n",
    "    # Estrazione delle coordinate dal dataset\n",
    "    times = dataset.time.values\n",
    "    levels = dataset.level.values\n",
    "    lats = dataset.latitude.values\n",
    "    lons = dataset.longitude.values\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    \n",
    "    results = {\n",
    "        'time': [],\n",
    "        'level': [],\n",
    "        'nuts3_id': [],\n",
    "        'nuts3_name': [],\n",
    "        f'mean_{variable_name}': []\n",
    "    }\n",
    "    \n",
    "    da = dataset[variable_name]\n",
    "    \n",
    "    # Itera per ogni combinazione di tempo (ora) e livello\n",
    "    for t_idx, t in enumerate(times):\n",
    "        for l_idx, l in enumerate(levels):\n",
    "            data_slice = da.isel(time=t_idx, level=l_idx).values\n",
    "            # Per ciascuna regione (NUTS-3)\n",
    "            for _, region in nuts3_gdf.iterrows():\n",
    "                nuts3_id = region['NUTS_ID']\n",
    "                nuts3_name = region['NUTS_NAME']\n",
    "                region_geom = region.geometry\n",
    "                \n",
    "                mask = shapely.vectorized.contains(region_geom, lon_grid, lat_grid)\n",
    "                mean_value = np.nanmean(data_slice[mask]) if np.any(mask) else np.nan\n",
    "                \n",
    "                results['time'].append(t)\n",
    "                results['level'].append(l)\n",
    "                results['nuts3_id'].append(nuts3_id)\n",
    "                results['nuts3_name'].append(nuts3_name)\n",
    "                results[f'mean_{variable_name}'].append(mean_value)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Carica il file geojson con i confini NUTS e filtra per l'Italia\n",
    "nuts = gpd.read_file(\"NUTS_RG_20M_2024_4326.geojson\")\n",
    "nuts = nuts[nuts[\"CNTR_CODE\"] == \"IT\"]\n",
    "\n",
    "# Aggrega i dati per ciascuna variabile\n",
    "res_pm10 = aggregate_by_nuts3(combined_data, nuts, 'pm10_conc')\n",
    "res_pm25 = aggregate_by_nuts3(combined_data, nuts, 'pm2p5_conc')\n",
    "res_co   = aggregate_by_nuts3(combined_data, nuts, 'co_conc')\n",
    "res_no2  = aggregate_by_nuts3(combined_data, nuts, 'no2_conc')\n",
    "res_so2  = aggregate_by_nuts3(combined_data, nuts, 'so2_conc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2aceb-303c-4ea0-baa2-50cbcbf1c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely.vectorized\n",
    "import os\n",
    "\n",
    "# Funzione per aggregare i dati per regioni NUTS-3 (o livello desiderato) usando la media\n",
    "def aggregate_by_nuts3(dataset, nuts3_gdf, variable_name='pm10_conc'):\n",
    "    times = dataset.time.values\n",
    "    levels = dataset.level.values\n",
    "    lats = dataset.latitude.values\n",
    "    lons = dataset.longitude.values\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    \n",
    "    results = {\n",
    "        'time': [],\n",
    "        'level': [],\n",
    "        'nuts3_id': [],\n",
    "        'nuts3_name': [],\n",
    "        f'mean_{variable_name}': []\n",
    "    }\n",
    "    \n",
    "    da = dataset[variable_name]\n",
    "    \n",
    "    for t_idx, t in enumerate(times):\n",
    "        for l_idx, l in enumerate(levels):\n",
    "            data_slice = da.isel(time=t_idx, level=l_idx).values\n",
    "            for _, region in nuts3_gdf.iterrows():\n",
    "                nuts3_id = region['NUTS_ID']\n",
    "                nuts3_name = region['NUTS_NAME']\n",
    "                region_geom = region.geometry\n",
    "                mask = shapely.vectorized.contains(region_geom, lon_grid, lat_grid)\n",
    "                mean_value = np.nanmean(data_slice[mask]) if np.any(mask) else np.nan\n",
    "                \n",
    "                results['time'].append(t)\n",
    "                results['level'].append(l)\n",
    "                results['nuts3_id'].append(nuts3_id)\n",
    "                results['nuts3_name'].append(nuts3_name)\n",
    "                results[f'mean_{variable_name}'].append(mean_value)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Funzione per creare una pivot table giornaliera\n",
    "def create_daily_pivot_and_format(res, var_prefix):\n",
    "    df = res.pivot_table(\n",
    "        index='nuts3_name',\n",
    "        columns='time',\n",
    "        values=f'mean_{var_prefix}_conc',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    # Se la variabile time è già in formato datetime, questo passaggio non è necessario.\n",
    "    # In caso contrario, bisogna adattarlo in base alla codifica temporale presente nei file .nc.\n",
    "    try:\n",
    "        df.columns = pd.to_datetime(df.columns)\n",
    "    except Exception:\n",
    "        baseline = pd.Timestamp(\"2024-01-01\")\n",
    "        df.columns = baseline + pd.to_timedelta(df.columns)\n",
    "    \n",
    "    # Raggruppa per data (troncando l'orario) e calcola la media giornaliera\n",
    "    df_daily = df.groupby(df.columns.date, axis=1).mean()\n",
    "    df_daily.columns = [pd.Timestamp(d).strftime('%d/%m/%Y') for d in df_daily.columns]\n",
    "    return df_daily\n",
    "\n",
    "# Cartella contenente i file .nc per ciascun mese\n",
    "output_dir = \"Italia_data\"\n",
    "nc_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".nc\")]\n",
    "\n",
    "# Carica il file geojson con i confini NUTS e filtra per l'Italia\n",
    "nuts = gpd.read_file(\"NUTS_RG_20M_2024_4326.geojson\")\n",
    "nuts = nuts[nuts[\"CNTR_CODE\"] == \"IT\"]\n",
    "\n",
    "# Elabora ogni file .nc separatamente\n",
    "for file in nc_files:\n",
    "    print(f\"Elaborazione di {file}...\")\n",
    "    ds = xr.open_dataset(file)\n",
    "    \n",
    "    # Aggrega per ciascuna variabile\n",
    "    res_pm10 = aggregate_by_nuts3(ds, nuts, 'pm10_conc')\n",
    "    res_pm25 = aggregate_by_nuts3(ds, nuts, 'pm2p5_conc')\n",
    "    res_co   = aggregate_by_nuts3(ds, nuts, 'co_conc')\n",
    "    res_no2  = aggregate_by_nuts3(ds, nuts, 'no2_conc')\n",
    "    res_so2  = aggregate_by_nuts3(ds, nuts, 'so2_conc')\n",
    "    \n",
    "    # Crea le tabelle pivot per ogni variabile con media giornaliera\n",
    "    df_pm25 = create_daily_pivot_and_format(res_pm25, 'pm2p5')\n",
    "    df_pm10 = create_daily_pivot_and_format(res_pm10, 'pm10')\n",
    "    df_co   = create_daily_pivot_and_format(res_co, 'co')\n",
    "    df_no2  = create_daily_pivot_and_format(res_no2, 'no2')\n",
    "    df_so2  = create_daily_pivot_and_format(res_so2, 'so2')\n",
    "    \n",
    "    # Estrae informazioni per denominare il file CSV (ad es. Italia_2024_01.nc)\n",
    "    month_info = os.path.basename(file).replace(\"Italia_\", \"\").replace(\".nc\", \"\")\n",
    "    \n",
    "    # Salva i DataFrame in file CSV per il mese corrente\n",
    "    df_pm25.to_csv(f\"Italia_pm25_daily_{month_info}.csv\")\n",
    "    df_pm10.to_csv(f\"Italia_pm10_daily_{month_info}.csv\")\n",
    "    df_co.to_csv(f\"Italia_co_daily_{month_info}.csv\")\n",
    "    df_no2.to_csv(f\"Italia_no2_daily_{month_info}.csv\")\n",
    "    df_so2.to_csv(f\"Italia_so2_daily_{month_info}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0d571-01d0-4bbc-8a29-e72c0e4e18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Lista degli inquinanti da elaborare\n",
    "pollutants = ['pm25', 'pm10', 'co', 'no2', 'so2']\n",
    "\n",
    "for pol in pollutants:\n",
    "    # Trova tutti i file CSV per l'inquinante corrente\n",
    "    csv_files = glob.glob(f\"Dati_italia_media/Italia_{pol}_daily_*.csv\")\n",
    "    df_list = []\n",
    "    \n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(f, index_col=0)\n",
    "        \n",
    "        # Estrai anno e mese dal nome del file, es: \"Italia_pm25_daily_2024_03.csv\"\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})\", f)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            month = int(match.group(2))\n",
    "            # Genera un range di date a partire dal primo giorno del mese, con il numero di giorni pari al numero di colonne\n",
    "            num_days = len(df.columns)\n",
    "            new_dates = pd.date_range(start=f\"{year}-{month:02d}-01\", periods=num_days, freq='D')\n",
    "            # Rinomina le colonne con il formato 'dd/mm/YYYY'\n",
    "            df.columns = new_dates.strftime('%d/%m/%Y')\n",
    "        else:\n",
    "            print(f\"Attenzione: impossibile estrarre anno/mese da {f}\")\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatena i DataFrame lungo le colonne\n",
    "    df_all = pd.concat(df_list, axis=1)\n",
    "    \n",
    "    # Ordina le colonne in ordine temporale:\n",
    "    # Converte le colonne in datetime, le ordina e poi riassegna il formato 'dd/mm/YYYY'\n",
    "    sorted_cols = sorted(df_all.columns, key=lambda x: pd.to_datetime(x, format='%d/%m/%Y'))\n",
    "    df_all = df_all.loc[:, sorted_cols]\n",
    "    \n",
    "    # Salva il risultato in un CSV per l'inquinante corrente\n",
    "    df_all.to_csv(f\"Italia_{pol}_daily_all.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b2322-c722-44de-b234-bb75510e2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely.vectorized\n",
    "import os\n",
    "\n",
    "# Crea una cartella per salvare i file NetCDF scaricati\n",
    "output_dir = \"Belgio_data\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Imposta il client CDS API\n",
    "client = cdsapi.Client(url='https://ads.atmosphere.copernicus.eu/api',\n",
    "                       key='6d4e1c06-ba15-4e08-a729-9f43ce76a3c1')\n",
    "\n",
    "# Parametri fissi\n",
    "dataset = \"cams-europe-air-quality-forecasts\"\n",
    "# Lista di tutte le ore della giornata\n",
    "hours = [f\"{i:02d}:00\" for i in range(24)]\n",
    "# Area geografica approssimativa per il Belgio [nord, ovest, sud, est]\n",
    "area = [51.5, 2.5, 49.5, 6.4]\n",
    "\n",
    "# Ciclo per ogni mese del 2024 (ad esempio, suddividendo per mese)\n",
    "months = pd.date_range(\"2024-01-01\", \"2024-12-01\", freq=\"MS\")\n",
    "for m in months:\n",
    "    start_date = m.strftime(\"%Y-%m-%d\")\n",
    "    end_date = (m + pd.offsets.MonthEnd(0)).strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    request = {\n",
    "        'variable': [ \n",
    "            'particulate_matter_2.5um', \n",
    "            'particulate_matter_10um',\n",
    "            'carbon_monoxide',\n",
    "            'nitrogen_dioxide',\n",
    "            'sulphur_dioxide'\n",
    "        ],\n",
    "        'model': ['ensemble'],\n",
    "        'level': ['0'],\n",
    "        'date': [f'{start_date}/{end_date}'],\n",
    "        'type': ['analysis'],\n",
    "        'time': hours,  # tutti gli orari della giornata\n",
    "        'leadtime_hour': ['0'],\n",
    "        'data_format': 'netcdf',\n",
    "        'area': area\n",
    "    }\n",
    "    \n",
    "    filename = os.path.join(output_dir, f\"Belgio_{m.strftime('%Y_%m')}.nc\")\n",
    "    print(f\"Recupero dati dal {start_date} al {end_date}...\")\n",
    "    client.retrieve(dataset, request).download(filename)\n",
    "\n",
    "# Aggrega tutti i file NetCDF in un unico dataset\n",
    "nc_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".nc\")]\n",
    "combined_data = xr.open_mfdataset(nc_files, combine='by_coords')\n",
    "\n",
    "# Funzione per aggregare i dati per regioni NUTS-3 usando la media\n",
    "def aggregate_by_nuts3(dataset, nuts3_gdf, variable_name='pm10_conc'):\n",
    "    times = dataset.time.values\n",
    "    levels = dataset.level.values\n",
    "    lats = dataset.latitude.values\n",
    "    lons = dataset.longitude.values\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    \n",
    "    results = {\n",
    "        'time': [],\n",
    "        'level': [],\n",
    "        'nuts3_id': [],\n",
    "        'nuts3_name': [],\n",
    "        f'mean_{variable_name}': []\n",
    "    }\n",
    "    \n",
    "    da = dataset[variable_name]\n",
    "    \n",
    "    # Per ogni combinazione di tempo (ora) e livello\n",
    "    for t_idx, t in enumerate(times):\n",
    "        for l_idx, l in enumerate(levels):\n",
    "            data_slice = da.isel(time=t_idx, level=l_idx).values\n",
    "            # Per ciascuna regione NUTS-3\n",
    "            for _, region in nuts3_gdf.iterrows():\n",
    "                nuts3_id = region['NUTS_ID']\n",
    "                nuts3_name = region['NUTS_NAME']\n",
    "                region_geom = region.geometry\n",
    "                \n",
    "                # Crea la maschera dei punti interni alla geometria\n",
    "                mask = shapely.vectorized.contains(region_geom, lon_grid, lat_grid)\n",
    "                mean_value = np.nanmean(data_slice[mask]) if np.any(mask) else np.nan\n",
    "                \n",
    "                results['time'].append(t)\n",
    "                results['level'].append(l)\n",
    "                results['nuts3_id'].append(nuts3_id)\n",
    "                results['nuts3_name'].append(nuts3_name)\n",
    "                results[f'mean_{variable_name}'].append(mean_value)\n",
    "                \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Carica il file geojson con i confini NUTS e filtra per il Belgio\n",
    "nuts = gpd.read_file(\"NUTS_RG_20M_2024_4326.geojson\")\n",
    "nuts = nuts[nuts[\"CNTR_CODE\"] == \"BE\"]\n",
    "\n",
    "# Aggrega i dati per ciascuna variabile\n",
    "res_pm10 = aggregate_by_nuts3(combined_data, nuts, 'pm10_conc')\n",
    "res_pm25 = aggregate_by_nuts3(combined_data, nuts, 'pm2p5_conc')\n",
    "res_co   = aggregate_by_nuts3(combined_data, nuts, 'co_conc')\n",
    "res_no2  = aggregate_by_nuts3(combined_data, nuts, 'no2_conc')\n",
    "res_so2  = aggregate_by_nuts3(combined_data, nuts, 'so2_conc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3e24b-1488-4f06-a9d7-a93d4e8838d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely.vectorized\n",
    "import os\n",
    "\n",
    "# Funzione per aggregare i dati per regioni NUTS-3 (o livello desiderato) usando la media\n",
    "def aggregate_by_nuts3(dataset, nuts3_gdf, variable_name='pm10_conc'):\n",
    "    times = dataset.time.values\n",
    "    levels = dataset.level.values\n",
    "    lats = dataset.latitude.values\n",
    "    lons = dataset.longitude.values\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    \n",
    "    results = {\n",
    "        'time': [],\n",
    "        'level': [],\n",
    "        'nuts3_id': [],\n",
    "        'nuts3_name': [],\n",
    "        f'mean_{variable_name}': []\n",
    "    }\n",
    "    \n",
    "    da = dataset[variable_name]\n",
    "    \n",
    "    for t_idx, t in enumerate(times):\n",
    "        for l_idx, l in enumerate(levels):\n",
    "            data_slice = da.isel(time=t_idx, level=l_idx).values\n",
    "            for _, region in nuts3_gdf.iterrows():\n",
    "                nuts3_id = region['NUTS_ID']\n",
    "                nuts3_name = region['NUTS_NAME']\n",
    "                region_geom = region.geometry\n",
    "                mask = shapely.vectorized.contains(region_geom, lon_grid, lat_grid)\n",
    "                mean_value = np.nanmean(data_slice[mask]) if np.any(mask) else np.nan\n",
    "                \n",
    "                results['time'].append(t)\n",
    "                results['level'].append(l)\n",
    "                results['nuts3_id'].append(nuts3_id)\n",
    "                results['nuts3_name'].append(nuts3_name)\n",
    "                results[f'mean_{variable_name}'].append(mean_value)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Funzione per creare una pivot table giornaliera\n",
    "def create_daily_pivot_and_format(res, var_prefix):\n",
    "    df = res.pivot_table(\n",
    "        index='nuts3_name',\n",
    "        columns='time',\n",
    "        values=f'mean_{var_prefix}_conc',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    # Se la variabile time è già in formato datetime, questo passaggio non è necessario.\n",
    "    # In caso contrario, bisogna adattarlo in base alla codifica temporale presente nei file .nc.\n",
    "    try:\n",
    "        df.columns = pd.to_datetime(df.columns)\n",
    "    except Exception:\n",
    "        baseline = pd.Timestamp(\"2024-01-01\")\n",
    "        df.columns = baseline + pd.to_timedelta(df.columns)\n",
    "    \n",
    "    # Raggruppa per data (troncando l'orario) e calcola la media giornaliera\n",
    "    df_daily = df.groupby(df.columns.date, axis=1).mean()\n",
    "    df_daily.columns = [pd.Timestamp(d).strftime('%d/%m/%Y') for d in df_daily.columns]\n",
    "    return df_daily\n",
    "\n",
    "# Cartella contenente i file .nc per ciascun mese per il Belgio\n",
    "output_dir = \"Belgio_data\"\n",
    "nc_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".nc\")]\n",
    "\n",
    "# Carica il file geojson con i confini NUTS e filtra per il Belgio\n",
    "nuts = gpd.read_file(\"NUTS_RG_20M_2024_4326.geojson\")\n",
    "nuts = nuts[nuts[\"CNTR_CODE\"] == \"BE\"]\n",
    "\n",
    "# Elabora ogni file .nc separatamente\n",
    "for file in nc_files:\n",
    "    print(f\"Elaborazione di {file}...\")\n",
    "    ds = xr.open_dataset(file)\n",
    "    \n",
    "    # Aggrega per ciascuna variabile\n",
    "    res_pm10 = aggregate_by_nuts3(ds, nuts, 'pm10_conc')\n",
    "    res_pm25 = aggregate_by_nuts3(ds, nuts, 'pm2p5_conc')\n",
    "    res_co   = aggregate_by_nuts3(ds, nuts, 'co_conc')\n",
    "    res_no2  = aggregate_by_nuts3(ds, nuts, 'no2_conc')\n",
    "    res_so2  = aggregate_by_nuts3(ds, nuts, 'so2_conc')\n",
    "    \n",
    "    # Crea le tabelle pivot per ogni variabile con media giornaliera\n",
    "    df_pm25 = create_daily_pivot_and_format(res_pm25, 'pm2p5')\n",
    "    df_pm10 = create_daily_pivot_and_format(res_pm10, 'pm10')\n",
    "    df_co   = create_daily_pivot_and_format(res_co, 'co')\n",
    "    df_no2  = create_daily_pivot_and_format(res_no2, 'no2')\n",
    "    df_so2  = create_daily_pivot_and_format(res_so2, 'so2')\n",
    "    \n",
    "    # Estrae informazioni per denominare il file CSV (ad es. Belgio_2024_01.nc)\n",
    "    month_info = os.path.basename(file).replace(\"Belgio_\", \"\").replace(\".nc\", \"\")\n",
    "    \n",
    "    # Salva i DataFrame in file CSV per il mese corrente\n",
    "    df_pm25.to_csv(f\"Belgio_pm25_daily_{month_info}.csv\")\n",
    "    df_pm10.to_csv(f\"Belgio_pm10_daily_{month_info}.csv\")\n",
    "    df_co.to_csv(f\"Belgio_co_daily_{month_info}.csv\")\n",
    "    df_no2.to_csv(f\"Belgio_no2_daily_{month_info}.csv\")\n",
    "    df_so2.to_csv(f\"Belgio_so2_daily_{month_info}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d91afe-9612-4cd4-9814-45dd05796db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Lista degli inquinanti da elaborare\n",
    "pollutants = ['pm25', 'pm10', 'co', 'no2', 'so2']\n",
    "\n",
    "for pol in pollutants:\n",
    "    # Trova tutti i file CSV per l'inquinante corrente nella cartella Belgio_data\n",
    "    csv_files = glob.glob(f\"Dati_belgio_media/Belgio_{pol}_daily_*.csv\")\n",
    "    df_list = []\n",
    "    \n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(f, index_col=0)\n",
    "        \n",
    "        # Estrai anno e mese dal nome del file, es: \"Belgio_pm25_daily_2024_03.csv\"\n",
    "        match = re.search(r\"(\\d{4})_(\\d{2})\", f)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            month = int(match.group(2))\n",
    "            # Genera un range di date a partire dal primo giorno del mese, con il numero di giorni pari al numero di colonne\n",
    "            num_days = len(df.columns)\n",
    "            new_dates = pd.date_range(start=f\"{year}-{month:02d}-01\", periods=num_days, freq='D')\n",
    "            # Rinomina le colonne con il formato 'dd/mm/YYYY'\n",
    "            df.columns = new_dates.strftime('%d/%m/%Y')\n",
    "        else:\n",
    "            print(f\"Attenzione: impossibile estrarre anno/mese da {f}\")\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatena i DataFrame lungo le colonne\n",
    "    df_all = pd.concat(df_list, axis=1)\n",
    "    \n",
    "    # Ordina le colonne in ordine temporale:\n",
    "    sorted_cols = sorted(df_all.columns, key=lambda x: pd.to_datetime(x, format='%d/%m/%Y'))\n",
    "    df_all = df_all.loc[:, sorted_cols]\n",
    "    \n",
    "    # Salva il risultato in un CSV per l'inquinante corrente\n",
    "    df_all.to_csv(f\"Belgio_{pol}_daily_all.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
